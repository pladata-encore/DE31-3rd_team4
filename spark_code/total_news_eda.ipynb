{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca25222-c966-44bf-a896-d7430568aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, json\n",
    "from datetime import datetime\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fee0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_hdfs(hdfs_info):\n",
    "    user = hdfs_info[\"user\"]\n",
    "    host = hdfs_info[\"host\"]\n",
    "    port = hdfs_info[\"port\"]\n",
    "    \n",
    "    try:\n",
    "        classpath = subprocess.Popen([hdfs_info[\"hdfs_path\"], \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "        os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "        hdfs = fs.HadoopFileSystem(host=hdfs_info[\"host\"], port=hdfs_info[\"port\"], user=hdfs_info[\"user\"])\n",
    "        \n",
    "        return hdfs\n",
    "    except Exception as e:\n",
    "        #print(f\"Failed to connect hdfs {user}@{host}:{port}\")\n",
    "        #log(f\"Failed to connect hdfs {user}@{host}:{port}\", 1)\n",
    "        return None\n",
    "\n",
    "\n",
    "def compare_file_date(filename):\n",
    "    # 파일명에서 날짜 추출 (예: kbs_2024-07-04_0012.csv)\n",
    "    try:\n",
    "        file_date_str = filename.split('_')[1]  # \"2024-07-04\"\n",
    "        file_date = datetime.strptime(file_date_str, \"%Y-%m-%d\").date()  # datetime.date(2024, 7, 4)\n",
    "        \n",
    "        # 현재 날짜 가져오기\n",
    "        current_date = datetime.now().date()  # 현재 날짜 (예: datetime.date(2024, 7, 4))\n",
    "        \n",
    "        # 날짜 비교\n",
    "        if file_date == current_date:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_file_list_from_hdfs(hdfs, hdfs_path):\n",
    "    # HDFS 경로에서 파일 목록 가져오기\n",
    "    try:\n",
    "        file_infos = hdfs.get_file_info(pa.fs.FileSelector(hdfs_path, recursive=False))\n",
    "        # if not file_info_list.is_directory:\n",
    "        #     raise Exception(f\"{hdfs_path} is not a directory\")\n",
    "        \n",
    "        file_list = [file_info.path for file_info in file_infos]\n",
    "        \n",
    "        return file_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file list from HDFS: {e}\")\n",
    "    \n",
    "def filter_files_by_date(file_list):\n",
    "    filtered_files = [file for file in file_list if compare_file_date(file)]\n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5feb06e5-767c-481e-abc1-2748966f86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_info_path = \"./API_KEYS/HDFS_INFO.json\"\n",
    "target_hdfs_dir_path = '/P3T5'\n",
    "with open(hdfs_info_path, 'r') as header_f:\n",
    "    hdfs_info = json.load(header_f)\n",
    "\n",
    "conf = pyspark.SparkConf() \\\n",
    "            .setAppName(\"hdfs2db\") \\\n",
    "            .setMaster(\"spark://master:7077\") \\\n",
    "            .set(\"spark.blockManager.port\", \"10025\") \\\n",
    "            .set(\"spark.driver.blockManager.port\", \"10026\") \\\n",
    "            .set(\"spark.driver.port\", \"10027\") \\\n",
    "            .set(\"spark.cores.max\", \"2\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "hdfs_connection = connect_hdfs(hdfs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743d3f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hdfs2db</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x792c83f96440>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d61677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7962801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting file list from HDFS: 'pyarrow._fs.FileInfo' object has no attribute 'is_directory'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m total_file_list \u001b[38;5;241m=\u001b[39m get_file_list_from_hdfs(hdfs_connection, target_hdfs_dir_path)\n\u001b[0;32m----> 2\u001b[0m target_file_list \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_files_by_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_file_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m, in \u001b[0;36mfilter_files_by_date\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_files_by_date\u001b[39m(file_list):\n\u001b[0;32m---> 55\u001b[0m     filtered_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_list \u001b[38;5;28;01mif\u001b[39;00m compare_file_date(file)]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_files\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "total_file_list = get_file_list_from_hdfs(hdfs_connection, target_hdfs_dir_path)\n",
    "target_file_list = filter_files_by_date(total_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8031172-da34-462f-8cc3-641f971ee605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if target_file_list:\n",
    "    df_list = [spark.read.csv(f\"hdfs://{target_hdfs_dir_path}/{file}\", header=True, inferSchema=True) for file in target_file_list]\n",
    "    combined_df = df_list[0]\n",
    "    for df in df_list[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    \n",
    "    # 결과 DataFrame 보여주기\n",
    "    combined_df.show()\n",
    "else:\n",
    "    print(\"No files found for the current date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd5eb3c9-8de0-486a-9263-4c6178e0b4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Contents: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Organizer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835271d2-129e-4d2c-9618-f9fbb5c312c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a7465-7ec0-44de-b8ff-7f91076ace08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
